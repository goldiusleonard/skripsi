{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "220401d2-ce01-4260-854d-8fbb884773ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"F:/skripsi/FAS-Skripsi-4\")\n",
    "\n",
    "from fas_simple_distill.model.divt.divt_mobilevit_v2 import DG_model\n",
    "from face_detection import FaceDetection, FaceSelectionMethod\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, HiResCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "sys.path.remove(\"F:/skripsi/FAS-Skripsi-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "859844d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mf:\\skripsi\\FAS-Skripsi-4\\evaluator\\cam\\gradcam-fas_divt.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m             x_crop \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(x_crop)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m x_crop\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m eval_transform \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     crop_align_face(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     T\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     T\u001b[39m.\u001b[39mNormalize(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m ])\n",
      "\u001b[1;32mf:\\skripsi\\FAS-Skripsi-4\\evaluator\\cam\\gradcam-fas_divt.ipynb Cell 2\u001b[0m in \u001b[0;36mcrop_align_face.__init__\u001b[1;34m(self, use_cuda, no_rotate, crop_size, scale, select_method)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     use_cuda: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     select_method \u001b[39m=\u001b[39m FaceSelectionMethod\u001b[39m.\u001b[39mAREA,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     use_onnx \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m use_cuda\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfd \u001b[39m=\u001b[39m FaceDetection(use_cuda, no_rotate, use_onnx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrop_size \u001b[39m=\u001b[39m crop_size\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/skripsi/FAS-Skripsi-4/evaluator/cam/gradcam-fas_divt.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39m=\u001b[39m scale\n",
      "File \u001b[1;32mc:\\Users\\goldi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\face_detection\\main.py:58\u001b[0m, in \u001b[0;36mFaceDetection.__init__\u001b[1;34m(self, use_cuda, no_rotate, use_onnx, **kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"Initialize face detection utility.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[39mBy default, ONNX based model inference will be used for better performance\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m        also set to `True`.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m use_onnx \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfdd \u001b[39m=\u001b[39m FaceDetector(use_cuda\u001b[39m=\u001b[39;49muse_cuda, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[39melif\u001b[39;00m use_onnx \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m use_cuda:\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfdd \u001b[39m=\u001b[39m FaceDetectorOnnx640(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\goldi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\face_detection\\retinaface.py:54\u001b[0m, in \u001b[0;36mFaceDetector.__init__\u001b[1;34m(self, use_cuda, top_k, nms_threshold, keep_top_k, target_size, max_size)\u001b[0m\n\u001b[0;32m     51\u001b[0m pretrain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet, checkpoint)\n\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> 54\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_scale \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m104\u001b[39m, \u001b[39m117\u001b[39m, \u001b[39m123\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior_data \u001b[39m=\u001b[39m priorbox(\n\u001b[0;32m     57\u001b[0m     min_sizes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mmin_sizes\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[0;32m     58\u001b[0m     steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[0;32m     59\u001b[0m     clip\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[0;32m     60\u001b[0m     image_size\u001b[39m=\u001b[39m(target_size, target_size)\n\u001b[0;32m     61\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\goldi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\goldi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goldi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 579 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\goldi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goldi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\goldi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "class crop_align_face:\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_cuda: bool = True,\n",
    "        no_rotate: bool = True,\n",
    "        crop_size: int = 256,\n",
    "        scale: float = 0.9,\n",
    "        select_method = FaceSelectionMethod.AREA,\n",
    "    ) -> None:\n",
    "        use_onnx = not use_cuda\n",
    "        self.fd = FaceDetection(use_cuda, no_rotate, use_onnx)\n",
    "        self.crop_size = crop_size\n",
    "        self.scale = scale\n",
    "        self.select_method = select_method\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, Image.Image):\n",
    "            input_is_pil = True\n",
    "            x = np.array(x)\n",
    "            x = cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            input_is_pil = False\n",
    "        \n",
    "        dets, angle = self.fd.predict(x)\n",
    "        x_crop, _ = self.fd.align_single_face(x, dets, angle, self.crop_size, self.scale, self.select_method)\n",
    "\n",
    "        if input_is_pil:\n",
    "            x_crop = cv2.cvtColor(x_crop, cv2.COLOR_BGR2RGB)\n",
    "            x_crop = Image.fromarray(x_crop)\n",
    "        \n",
    "        return x_crop\n",
    "\n",
    "eval_transform = T.Compose([\n",
    "    crop_align_face(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGlobDataset(Dataset):\n",
    "    def __init__(self, root, glob_patt, transform=None):\n",
    "        self.root = root\n",
    "        self.glob_patt = glob_patt\n",
    "        self.images = sorted(list(Path(root).rglob(glob_patt)))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images[index]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, img_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2199453",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"casia_mfsd_FP\"\n",
    "model_name = \"divt_mobilevits_ICMtoO\"\n",
    "device = \"cuda\"\n",
    "camtype = \"gradcam\"\n",
    "target_class = 0\n",
    "\n",
    "dataset = SimpleGlobDataset(\n",
    "    root=f\"F:/skripsi/FAS-Skripsi-4/results/divt_mobilevits_ICMtoO/oulu_npu/FP\",\n",
    "    glob_patt=\"*.png\",\n",
    "    transform=eval_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1edad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DG_model(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(in_channels=16, out_channels=32, stride=1, exp=4, dilation=1, skip_conn=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(in_channels=32, out_channels=64, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "      (1): InvertedResidual(in_channels=64, out_channels=64, stride=1, exp=4, dilation=1, skip_conn=True)\n",
       "      (2): InvertedResidual(in_channels=64, out_channels=64, stride=1, exp=4, dilation=1, skip_conn=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(in_channels=64, out_channels=96, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "      (1): MobileViTBlock(\n",
       "      \t Local representations\n",
       "      \t\t Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      \t\t Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      \t Global representations with patch size of 2x2\n",
       "      \t\t TransformerEncoder(embed_dim=144, ffn_dim=288, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t TransformerEncoder(embed_dim=144, ffn_dim=288, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      \t\t Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      \t Feature fusion\n",
       "      \t\t Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(in_channels=96, out_channels=128, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "      (1): MobileViTBlock(\n",
       "      \t Local representations\n",
       "      \t\t Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      \t\t Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      \t Global representations with patch size of 2x2\n",
       "      \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      \t\t Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      \t Feature fusion\n",
       "      \t\t Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(in_channels=128, out_channels=160, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "      (1): MobileViTBlock(\n",
       "      \t Local representations\n",
       "      \t\t Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      \t\t Conv2d(160, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      \t Global representations with patch size of 2x2\n",
       "      \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "      \t\t LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "      \t\t Conv2d(240, 160, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      \t Feature fusion\n",
       "      \t\t Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "      )\n",
       "    )\n",
       "    (6): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (classifier_layer1): Linear(in_features=640, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DG_model(\"F:/skripsi/FAS-Skripsi-4/fas_simple_distill/model/mobilevit_config/mobilevit_s.yaml\")\n",
    "ckpt = torch.load(f\"F:/skripsi/FAS-Skripsi-4/evaluator/weights/{model_name}.pth\")\n",
    "state_dict = ckpt['model']\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# model = ModelWrapper(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if camtype == \"gradcam++\":\n",
    "    if target_class == 0:\n",
    "        dst_path = f\"./results/gradcam++/spoof_map/{model_name}-{dataset_name}\"\n",
    "    else:\n",
    "        dst_path = f\"./results/gradcam++/live_map/{model_name}-{dataset_name}\"\n",
    "elif camtype == \"gradcam\":\n",
    "    if target_class == 0:\n",
    "        dst_path = f\"./results/gradcam/spoof_map/{model_name}-{dataset_name}\"\n",
    "    else:\n",
    "        dst_path = f\"./results/gradcam/live_map/{model_name}-{dataset_name}\"\n",
    "elif camtype == \"hirescam\":\n",
    "    if target_class == 0:\n",
    "        dst_path = f\"./results/hirescam/spoof_map/{model_name}-{dataset_name}\"\n",
    "    else:\n",
    "        dst_path = f\"./results/hirescam/live_map/{model_name}-{dataset_name}\"\n",
    "else:\n",
    "    raise RuntimeError(\"camtype should be either 'gradcam', 'gradcam++', or 'hirescam'\")\n",
    "\n",
    "if not os.path.exists(dst_path):\n",
    "    os.makedirs(dst_path)\n",
    "dst_path = Path(dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa246e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "  (1): Sequential(\n",
       "    (0): InvertedResidual(in_channels=16, out_channels=32, stride=1, exp=4, dilation=1, skip_conn=False)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): InvertedResidual(in_channels=32, out_channels=64, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "    (1): InvertedResidual(in_channels=64, out_channels=64, stride=1, exp=4, dilation=1, skip_conn=True)\n",
       "    (2): InvertedResidual(in_channels=64, out_channels=64, stride=1, exp=4, dilation=1, skip_conn=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): InvertedResidual(in_channels=64, out_channels=96, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "    (1): MobileViTBlock(\n",
       "    \t Local representations\n",
       "    \t\t Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    \t\t Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    \t Global representations with patch size of 2x2\n",
       "    \t\t TransformerEncoder(embed_dim=144, ffn_dim=288, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t TransformerEncoder(embed_dim=144, ffn_dim=288, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "    \t\t Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    \t Feature fusion\n",
       "    \t\t Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    )\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): InvertedResidual(in_channels=96, out_channels=128, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "    (1): MobileViTBlock(\n",
       "    \t Local representations\n",
       "    \t\t Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    \t\t Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    \t Global representations with patch size of 2x2\n",
       "    \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    \t\t Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    \t Feature fusion\n",
       "    \t\t Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): InvertedResidual(in_channels=128, out_channels=160, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "    (1): MobileViTBlock(\n",
       "    \t Local representations\n",
       "    \t\t Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    \t\t Conv2d(160, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    \t Global representations with patch size of 2x2\n",
       "    \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "    \t\t LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "    \t\t Conv2d(240, 160, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    \t Feature fusion\n",
       "    \t\t Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "    )\n",
       "  )\n",
       "  (6): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_model = model.backbone\n",
    "main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6a59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish),\n",
       " Sequential(\n",
       "   (0): InvertedResidual(in_channels=16, out_channels=32, stride=1, exp=4, dilation=1, skip_conn=False)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): InvertedResidual(in_channels=32, out_channels=64, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "   (1): InvertedResidual(in_channels=64, out_channels=64, stride=1, exp=4, dilation=1, skip_conn=True)\n",
       "   (2): InvertedResidual(in_channels=64, out_channels=64, stride=1, exp=4, dilation=1, skip_conn=True)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): InvertedResidual(in_channels=64, out_channels=96, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "   (1): MobileViTBlock(\n",
       "   \t Local representations\n",
       "   \t\t Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   \t\t Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   \t Global representations with patch size of 2x2\n",
       "   \t\t TransformerEncoder(embed_dim=144, ffn_dim=288, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t TransformerEncoder(embed_dim=144, ffn_dim=288, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "   \t\t Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   \t Feature fusion\n",
       "   \t\t Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): InvertedResidual(in_channels=96, out_channels=128, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "   (1): MobileViTBlock(\n",
       "   \t Local representations\n",
       "   \t\t Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   \t\t Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   \t Global representations with patch size of 2x2\n",
       "   \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t TransformerEncoder(embed_dim=192, ffn_dim=384, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "   \t\t Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   \t Feature fusion\n",
       "   \t\t Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): InvertedResidual(in_channels=128, out_channels=160, stride=2, exp=4, dilation=1, skip_conn=False)\n",
       "   (1): MobileViTBlock(\n",
       "   \t Local representations\n",
       "   \t\t Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   \t\t Conv2d(160, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   \t Global representations with patch size of 2x2\n",
       "   \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t TransformerEncoder(embed_dim=240, ffn_dim=480, dropout=0.1, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=Swish, norm_fn=layer_norm)\n",
       "   \t\t LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "   \t\t Conv2d(240, 160, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   \t Feature fusion\n",
       "   \t\t Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)\n",
       "   )\n",
       " ),\n",
       " Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=Swish)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_layers = [main_model.backbone[1]]\n",
    "target_layers = []\n",
    "\n",
    "for layer in main_model:\n",
    "    target_layers.append(layer)\n",
    "target_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_transform = None\n",
    "if camtype == \"gradcam++\":\n",
    "    cam = GradCAMPlusPlus(model=model, target_layers=target_layers, use_cuda=True, reshape_transform=reshape_transform)\n",
    "elif camtype == \"gradcam\":\n",
    "    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True, reshape_transform=reshape_transform)\n",
    "elif camtype == \"hirescam\":\n",
    "    cam = HiResCAM(model=model, target_layers=target_layers, use_cuda=True, reshape_transform=reshape_transform)\n",
    "else:\n",
    "    raise RuntimeError(\"camtype should be either 'gradcam', 'gradcam++', or 'hirescam'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43572213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd07c2701186438c8be3c1b8407d9dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TARGET_CLASS = [ClassifierOutputTarget(target_class)]\n",
    "crop_face = crop_align_face()\n",
    "\n",
    "for imgten, imgpath in tqdm(dataset):\n",
    "    grayscale_cam = cam(input_tensor=imgten[None, ...], targets=TARGET_CLASS)\n",
    "\n",
    "    cam_map = grayscale_cam[0]\n",
    "    \n",
    "    img_ori = Image.open(imgpath).convert(\"RGB\")\n",
    "    img_ori = crop_face(img_ori)\n",
    "    img_ori = np.asarray(img_ori) / 255.0\n",
    "    img_cam = Image.fromarray(show_cam_on_image(img_ori, cam_map, use_rgb=True))\n",
    "\n",
    "    save_path = dst_path.joinpath(*imgpath.parts[-2:])\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    img_cam.save(save_path.with_suffix(\".jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e04218f22d6b8fe5ad49aba9f5ef8b780b14060a2b6c9db55982c4ec8f62d1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
